{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a4b591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import os\n",
    "import great_expectations as gx\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37227a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the spark session with the passed configs.\n",
    "    \"\"\"\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"How-Desafio-03\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a6ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:35:14 WARN Utils: Your hostname, 14111-NB resolves to a loopback address: 127.0.1.1; using 172.18.58.55 instead (on interface eth0)\n",
      "24/02/06 20:35:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 20:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c624bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "weather_df = (spark.read\n",
    "                  .option(\"inferSchema\", True)\n",
    "                  .json(\"./*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77ce101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(calctime=0.003620315, city='belem', city_id=1, cnt=1, cod='200', latitude='-1.45056', list=[Row(clouds=Row(all=0), dt=1706230800, main=Row(feels_like=299.17, humidity=94, pressure=1012, temp=299.17, temp_max=299.17, temp_min=299.17), weather=[Row(description='clear sky', icon='01n', id=800, main='Clear')], wind=Row(deg=40, speed=1.54))], longitude='-48.4682453', message='Count: 1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbff9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(input_dataset):\n",
    "    \n",
    "    gx_context = gx.get_context()\n",
    "    datasource = gx_context.sources.add_spark(\"my_spark_datasource\")\n",
    "\n",
    "    data_asset = datasource.add_dataframe_asset(name=\"my_df_asset\", dataframe=input_dataset).build_batch_request()\n",
    "    \n",
    "    gx_context.add_or_update_expectation_suite(\"my_expectation_suite\")\n",
    "    \n",
    "    #my_batch_request = data_asset\n",
    "    \n",
    "    validator = gx_context.get_validator(\n",
    "    batch_request=data_asset,\n",
    "    expectation_suite_name=\"my_expectation_suite\"\n",
    "                                        )\n",
    "    \n",
    "    weather_null = validator.expect_column_values_to_not_be_null(column=\"city\")\n",
    "    date_format = validator.expect_column_values_to_match_strftime_format(\"date_partition\", \"%Y-%m-%d\")\n",
    "    rows_number = validator.expect_table_row_count_to_be_between(27,27)\n",
    "\n",
    "    \n",
    "    if weather_null.success == False :\n",
    "      raise ValueError(f\"Data quality check failed {weather_null.expectation_config.kwargs['column']} is null.\")\n",
    "    else : logger.info(f\"Data quality check success {weather_null.expectation_config.kwargs['column']} is not null.\")\n",
    "    \n",
    "    if date_format.success == False :\n",
    "      raise ValueError(f\"Data quality check failed {date_format.expectation_config.kwargs['column']} is in the wrong format.\")\n",
    "    else: logger.info(f\"Data quality check success {date_format.expectation_config.kwargs['column']}  is in the right format.\")\n",
    "        \n",
    "    if rows_number.success == False :\n",
    "      raise ValueError(f\"Data quality check failed, dataset has unexpected number of rows.\")\n",
    "    else: logger.info(f\"Data quality check success, dataset has the expected number of rows.\")\n",
    "       \n",
    "    logger.info(f\"All validators passed with success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8ebab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- weather.parquet completed ---\n"
     ]
    }
   ],
   "source": [
    "weather_df_partition = weather_df.withColumn('date_partition', from_unixtime(col(\"list.dt\")[0],\"yyyy-MM-dd\"))\n",
    "weather_df_partition.collect()\n",
    "\n",
    "#data_quality(weather_df_partition)\n",
    "\n",
    "weather_df_partition.write.partitionBy('date_partition').parquet(os.path.join('', 'weather'), 'overwrite')\n",
    "\n",
    "print(\"--- weather.parquet completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011c762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.util:Could not find local context root directory\n",
      "INFO:great_expectations.data_context.types.base:Created temporary directory '/tmp/tmp012cn0lk' for ephemeral docs site\n",
      "INFO:great_expectations.data_context.data_context.abstract_data_context:EphemeralDataContext has not implemented `_load_fluent_config()` returning empty `GxConfig`\n",
      "INFO:great_expectations.datasource.fluent.config:Loading 'datasources' ->\n",
      "[]\n",
      "INFO:great_expectations.datasource.fluent.fluent_base_model:SparkDatasource.dict() - substituting config values\n",
      "24/02/06 20:41:52 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a119452d8f9a406e9b22ca7ab67ce5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144c769587744a75a028ef8a4ab8ca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddae276cef4543f4911d4932f293a5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data quality check success city is not null.\n",
      "INFO:__main__:Data quality check success date_partition  is in the right format.\n",
      "INFO:__main__:Data quality check success, dataset has the expected number of rows.\n",
      "INFO:__main__:All validators passed with success!\n"
     ]
    }
   ],
   "source": [
    "weather_business = (weather_trusted.select(col('list')[0].alias('list_0'), 'city', 'latitude', 'longitude', 'date_partition')\n",
    "                                   .select('city',\n",
    "                                           'latitude',\n",
    "                                           'longitude',                                           \n",
    "                                           col('list_0.weather.main')[0].alias('main_weather'),\n",
    "                                           col('list_0.weather.description')[0].alias('main_weather_description'),\n",
    "                                           'list_0.main.temp',\n",
    "                                           'list_0.main.feels_like',\n",
    "                                           'list_0.main.pressure',\n",
    "                                           'list_0.main.humidity',\n",
    "                                           'list_0.main.temp_min',\n",
    "                                           'list_0.main.temp_max',\n",
    "                                           'list_0.wind.speed',\n",
    "                                           'list_0.wind.deg',\n",
    "                                           col('list_0.clouds.all').alias('clouds'),\n",
    "                                           col('list_0.dt').alias('collect_timestamp'),\n",
    "                                          col('date_partition').cast(StringType()))\n",
    "                   )\n",
    "\n",
    "data_quality(weather_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0660e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

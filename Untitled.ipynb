{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a4b591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import os\n",
    "import great_expectations as gx\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37227a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the spark session with the passed configs.\n",
    "    \"\"\"\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"How-Desafio-03\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a6ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:35:14 WARN Utils: Your hostname, 14111-NB resolves to a loopback address: 127.0.1.1; using 172.18.58.55 instead (on interface eth0)\n",
      "24/02/06 20:35:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 20:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c624bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "weather_df = (spark.read\n",
    "                  .option(\"inferSchema\", True)\n",
    "                  .json(\"./*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77ce101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(calctime=0.003620315, city='belem', city_id=1, cnt=1, cod='200', latitude='-1.45056', list=[Row(clouds=Row(all=0), dt=1706230800, main=Row(feels_like=299.17, humidity=94, pressure=1012, temp=299.17, temp_max=299.17, temp_min=299.17), weather=[Row(description='clear sky', icon='01n', id=800, main='Clear')], wind=Row(deg=40, speed=1.54))], longitude='-48.4682453', message='Count: 1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbff9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(input_dataset):\n",
    "    \n",
    "    gx_context = gx.get_context()\n",
    "    datasource = gx_context.sources.add_spark(\"my_spark_datasource\")\n",
    "\n",
    "    data_asset = datasource.add_dataframe_asset(name=\"my_df_asset\", dataframe=input_dataset).build_batch_request()\n",
    "    \n",
    "    gx_context.add_or_update_expectation_suite(\"my_expectation_suite\")\n",
    "    \n",
    "    #my_batch_request = data_asset\n",
    "    \n",
    "    validator = gx_context.get_validator(\n",
    "    batch_request=data_asset,\n",
    "    expectation_suite_name=\"my_expectation_suite\"\n",
    "                                        )\n",
    "    \n",
    "    weather_null = validator.expect_column_values_to_not_be_null(column=\"city\")\n",
    "    date_format = validator.expect_column_values_to_match_strftime_format(\"date_partition\", \"%Y-%m-%d\")\n",
    "    rows_number = validator.expect_table_row_count_to_be_between(27,27)\n",
    "\n",
    "    \n",
    "    if weather_null.success == False :\n",
    "      raise ValueError(f\"Data quality check failed {weather_null.expectation_config.kwargs['column']} is null.\")\n",
    "    else : logger.info(f\"Data quality check success {weather_null.expectation_config.kwargs['column']} is not null.\")\n",
    "    \n",
    "    if date_format.success == False :\n",
    "      raise ValueError(f\"Data quality check failed {date_format.expectation_config.kwargs['column']} is in the wrong format.\")\n",
    "    else: logger.info(f\"Data quality check success {date_format.expectation_config.kwargs['column']}  is in the right format.\")\n",
    "        \n",
    "    if rows_number.success == False :\n",
    "      raise ValueError(f\"Data quality check failed, dataset has unexpected number of rows.\")\n",
    "    else: logger.info(f\"Data quality check success, dataset has the expected number of rows.\")\n",
    "       \n",
    "    logger.info(f\"All validators passed with success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8ebab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- weather.parquet completed ---\n"
     ]
    }
   ],
   "source": [
    "weather_df_partition = weather_df.withColumn('date_partition', from_unixtime(col(\"list.dt\")[0],\"yyyy-MM-dd\"))\n",
    "weather_df_partition.collect()\n",
    "\n",
    "#data_quality(weather_df_partition)\n",
    "\n",
    "weather_df_partition.write.partitionBy('date_partition').parquet(os.path.join('', 'weather'), 'overwrite')\n",
    "\n",
    "print(\"--- weather.parquet completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011c762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.util:Could not find local context root directory\n",
      "INFO:great_expectations.data_context.types.base:Created temporary directory '/tmp/tmp012cn0lk' for ephemeral docs site\n",
      "INFO:great_expectations.data_context.data_context.abstract_data_context:EphemeralDataContext has not implemented `_load_fluent_config()` returning empty `GxConfig`\n",
      "INFO:great_expectations.datasource.fluent.config:Loading 'datasources' ->\n",
      "[]\n",
      "INFO:great_expectations.datasource.fluent.fluent_base_model:SparkDatasource.dict() - substituting config values\n",
      "24/02/06 20:41:52 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a119452d8f9a406e9b22ca7ab67ce5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144c769587744a75a028ef8a4ab8ca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddae276cef4543f4911d4932f293a5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data quality check success city is not null.\n",
      "INFO:__main__:Data quality check success date_partition  is in the right format.\n",
      "INFO:__main__:Data quality check success, dataset has the expected number of rows.\n",
      "INFO:__main__:All validators passed with success!\n"
     ]
    }
   ],
   "source": [
    "weather_business = (weather_trusted.select(col('list')[0].alias('list_0'), 'city', 'latitude', 'longitude', 'date_partition')\n",
    "                                   .select('city',\n",
    "                                           'latitude',\n",
    "                                           'longitude',                                           \n",
    "                                           col('list_0.weather.main')[0].alias('main_weather'),\n",
    "                                           col('list_0.weather.description')[0].alias('main_weather_description'),\n",
    "                                           'list_0.main.temp',\n",
    "                                           'list_0.main.feels_like',\n",
    "                                           'list_0.main.pressure',\n",
    "                                           'list_0.main.humidity',\n",
    "                                           'list_0.main.temp_min',\n",
    "                                           'list_0.main.temp_max',\n",
    "                                           'list_0.wind.speed',\n",
    "                                           'list_0.wind.deg',\n",
    "                                           col('list_0.clouds.all').alias('clouds'),\n",
    "                                           col('list_0.dt').alias('collect_timestamp'),\n",
    "                                          col('date_partition').cast(StringType()))\n",
    "                   )\n",
    "\n",
    "data_quality(weather_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a0660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b60c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the spark session with the passed configs.\n",
    "    \"\"\"\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"How-Desafio-3\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73da0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(bucket):\n",
    "    files = []\n",
    "    s3 = boto3.client('s3')\n",
    "    result = s3.list_objects(Bucket=bucket)\n",
    "    for obj in result['Contents']:\n",
    "        files.append(obj['Key'])\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1196abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidToken) when calling the ListObjects operation: The provided token is malformed or otherwise invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m create_spark_session()\n\u001b[1;32m      2\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow-desafio-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#recent_file = recent_date_files(files)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mlist_files\u001b[0;34m(bucket)\u001b[0m\n\u001b[1;32m      2\u001b[0m files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContents\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     files\u001b[38;5;241m.\u001b[39mappend(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidToken) when calling the ListObjects operation: The provided token is malformed or otherwise invalid."
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "bucket = \"how-desafio-3\"\n",
    "\n",
    "files = list_files(bucket)\n",
    "#recent_file = recent_date_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2490a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
